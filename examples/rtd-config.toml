type = "rtd"

[task]
mask_ratio = 0.15  # The proportion of modified tokens to mask
embeddings_sharing = "electra"

[tuning]
# The default MLM fine-tuning config
batch_size = 64
betas = [0.9, 0.98]
epsilon = 1e-8
learning_rate = 1e-4
# Uncomment these for a more complex training setup
# lr_decay_steps = 1000000
# warmup_steps = 1000
#Â weight_decay = 1e-5