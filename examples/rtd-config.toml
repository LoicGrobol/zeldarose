type = "rtd"

[task]
mask_ratio = 0.15  # The proportion of modified tokens to mask
embeddings_sharing = "electra"

[tuning]
# The default MLM fine-tuning config
batch_size = 32
betas = [0.9, 0.999]
epsilon = 1e-6
learning_rate = 3e-4
# Uncomment these for a more complex training setup
# lr_decay_steps = 1000000
# warmup_steps = 1000
#Â weight_decay = 1e-5